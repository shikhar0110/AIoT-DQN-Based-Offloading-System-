{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1840790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AIoT-enhanced training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shikh\\AppData\\Local\\Temp\\ipykernel_19220\\2407729336.py:110: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  state_batch = torch.FloatTensor(batch.state).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50/500 | Max Latency: 6199.59 ms | Avg Reward: -38057.51 | Epsilon: 0.010\n",
      "Episode 100/500 | Max Latency: 8184.86 ms | Avg Reward: -39084.65 | Epsilon: 0.010\n",
      "Episode 150/500 | Max Latency: 3521.50 ms | Avg Reward: -31184.97 | Epsilon: 0.010\n",
      "Episode 200/500 | Max Latency: 3257.87 ms | Avg Reward: -19248.90 | Epsilon: 0.010\n",
      "Episode 250/500 | Max Latency: 4779.72 ms | Avg Reward: -33061.05 | Epsilon: 0.010\n",
      "Episode 300/500 | Max Latency: 2942.15 ms | Avg Reward: -28367.73 | Epsilon: 0.010\n",
      "Episode 350/500 | Max Latency: 9664.99 ms | Avg Reward: -48402.94 | Epsilon: 0.010\n",
      "Episode 400/500 | Max Latency: 2872.71 ms | Avg Reward: -27866.50 | Epsilon: 0.010\n",
      "Episode 450/500 | Max Latency: 7667.92 ms | Avg Reward: -31305.34 | Epsilon: 0.010\n",
      "Episode 500/500 | Max Latency: 7983.87 ms | Avg Reward: -64655.31 | Epsilon: 0.010\n",
      "\n",
      "Starting AIoT-enhanced evaluation...\n",
      "Evaluation Trial 1/10 | Max Latency: 10549.36 ms\n",
      "Evaluation Trial 2/10 | Max Latency: 5195.06 ms\n",
      "Evaluation Trial 3/10 | Max Latency: 5563.68 ms\n",
      "Evaluation Trial 4/10 | Max Latency: 6972.00 ms\n",
      "Evaluation Trial 5/10 | Max Latency: 6692.78 ms\n",
      "Evaluation Trial 6/10 | Max Latency: 9585.36 ms\n",
      "Evaluation Trial 7/10 | Max Latency: 19670.46 ms\n",
      "Evaluation Trial 8/10 | Max Latency: 7392.50 ms\n",
      "Evaluation Trial 9/10 | Max Latency: 4075.88 ms\n",
      "Evaluation Trial 10/10 | Max Latency: 4004.16 ms\n",
      "\n",
      "Final AIoT-DQN Performance:\n",
      "Average Max Latency: 7970.12 ± 4398.94 ms\n",
      "Best Trial: 4004.16 ms\n",
      "Worst Trial: 19670.46 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# ===== AIoT Paper Components =====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'priority'))\n",
    "\n",
    "class Server:\n",
    "    def __init__(self, server_id, processing_speed, transmission_delay):\n",
    "        self.id = server_id\n",
    "        self.processing_speed = processing_speed  # From AIoT paper Eq.9\n",
    "        self.transmission_delay = transmission_delay\n",
    "        self.queues = {'high': deque(), 'low': deque()}  # Priority queues (AIoT Eq.3-8)\n",
    "        self.current_time = 0.0\n",
    "\n",
    "    def add_task(self, task_size, arrival_time, priority):\n",
    "        \"\"\"AIoT-enhanced task addition with priority handling\"\"\"\n",
    "        processing_time = task_size / self.processing_speed\n",
    "        last_end = max([q[-1][1] for q in self.queues.values() if q]) if any(self.queues.values()) else self.current_time\n",
    "        start_time = max(arrival_time + self.transmission_delay, last_end)\n",
    "        end_time = start_time + processing_time\n",
    "        self.queues[priority].append((task_size, end_time))\n",
    "        self.current_time = max(self.current_time, end_time)\n",
    "        return end_time - arrival_time\n",
    "\n",
    "    def reset(self):\n",
    "        self.queues['high'].clear()\n",
    "        self.queues['low'].clear()\n",
    "        self.current_time = 0.0\n",
    "\n",
    "class AIoT_DQN(nn.Module):\n",
    "    \"\"\"Enhanced DQN architecture from AIoT paper\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(AIoT_DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class AIoT_DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        # AIoT parameters\n",
    "        self.ω_H = 100  # High-priority weight (Table II)\n",
    "        self.ω_L = 1    # Low-priority weight\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "        # Double DQN implementation\n",
    "        self.policy_net = AIoT_DQN(state_size, action_size).to(device)\n",
    "        self.target_net = AIoT_DQN(state_size, action_size).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)\n",
    "        self.loss_fn = nn.HuberLoss()  # AIoT Eq.32\n",
    "\n",
    "    def get_state(self, task_size, servers):\n",
    "        \"\"\"Enhanced state space (AIoT Eq.17 + server capabilities)\"\"\"\n",
    "        state = [task_size]\n",
    "        for s in servers:\n",
    "            state += [\n",
    "                len(s.queues['high']), \n",
    "                len(s.queues['low']),\n",
    "                s.processing_speed,\n",
    "                s.transmission_delay,\n",
    "                np.random.rand()  # Simulated channel gain\n",
    "            ]\n",
    "        return np.array(state, dtype=np.float32)\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def remember(self, state, action, next_state, reward, priority):\n",
    "        self.memory.append(Transition(state, action, next_state, reward, priority))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        transitions = random.sample(self.memory, self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # Priority-aware experience replay\n",
    "        weights = torch.tensor([\n",
    "            1.0 if t.priority == 'high' else 0.3 for t in transitions\n",
    "        ], dtype=torch.float32).to(device)\n",
    "        \n",
    "        state_batch = torch.FloatTensor(batch.state).to(device)\n",
    "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).to(device)\n",
    "        next_state_batch = torch.FloatTensor(batch.next_state).to(device)\n",
    "        \n",
    "        current_q = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        next_q = self.target_net(next_state_batch).max(1)[0].detach()\n",
    "        expected_q = reward_batch + (self.gamma * next_q)\n",
    "        \n",
    "        loss = self.loss_fn(current_q.squeeze(), expected_q)\n",
    "        loss = (weights * loss).mean()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "\n",
    "# ===== Simulation Functions =====\n",
    "def generate_tasks(n_tasks, priority_ratio=0.3):\n",
    "    tasks = []\n",
    "    for _ in range(n_tasks):\n",
    "        size = np.random.randint(1, 100)\n",
    "        arrival = np.random.exponential(scale=1/10)\n",
    "        priority = 'high' if random.random() < priority_ratio else 'low'\n",
    "        tasks.append((size, arrival, priority))\n",
    "    return sorted(tasks, key=lambda x: x[1])\n",
    "\n",
    "def run_simulation(agent, servers, tasks, is_training=True):\n",
    "    total_time = 0.0\n",
    "    for task_size, arrival_time, priority in tasks:\n",
    "        state = agent.get_state(task_size, servers)\n",
    "        action = agent.act(state)\n",
    "        selected_server = servers[action]\n",
    "        \n",
    "        latency = selected_server.add_task(task_size, arrival_time, priority)\n",
    "        reward = - (agent.ω_H * latency if priority == 'high' else agent.ω_L * latency)\n",
    "        next_state = agent.get_state(task_size, servers)\n",
    "        \n",
    "        if is_training:\n",
    "            agent.remember(state, action, next_state, reward, priority)\n",
    "            agent.replay()\n",
    "            if len(agent.memory) % 100 == 0:\n",
    "                agent.update_target_network()\n",
    "        \n",
    "        total_time = max(total_time, latency)\n",
    "    return total_time\n",
    "\n",
    "# ===== Main Execution =====\n",
    "def main():\n",
    "    # Training Parameters\n",
    "    n_episodes = 500\n",
    "    n_tasks = 1600\n",
    "    n_eval_trials = 10\n",
    "    \n",
    "    # Initialize components\n",
    "    servers = [Server(i, np.random.randint(1,10), np.random.randint(2,20)) \n",
    "              for i in range(10)]\n",
    "    agent = AIoT_DQNAgent(state_size=51, action_size=10)  # 1 + 10*(4+1)\n",
    "\n",
    "    # Training Phase\n",
    "    print(\"Starting AIoT-enhanced training...\")\n",
    "    for episode in range(n_episodes):\n",
    "        # Reset environment\n",
    "        servers = [Server(i, np.random.randint(1,10), np.random.randint(2,20)) \n",
    "                  for i in range(10)]\n",
    "        tasks = generate_tasks(n_tasks)\n",
    "        \n",
    "        max_latency = run_simulation(agent, servers, tasks)\n",
    "        \n",
    "        # ==== CORRECTED SECTION ====\n",
    "        # Convert deque to list for slicing\n",
    "        memory_list = list(agent.memory)\n",
    "        recent_transitions = memory_list[-n_tasks:] if len(memory_list) >= n_tasks else memory_list\n",
    "        # ===========================\n",
    "        \n",
    "        total_reward = sum(t.reward for t in recent_transitions)\n",
    "        avg_reward = total_reward / len(recent_transitions) if recent_transitions else 0\n",
    "\n",
    "        if (episode+1) % 50 == 0:\n",
    "            print(f\"Episode {episode+1}/{n_episodes} | \"\n",
    "                  f\"Max Latency: {max_latency:.2f} ms | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "    # Evaluation Phase\n",
    "    print(\"\\nStarting AIoT-enhanced evaluation...\")\n",
    "    eval_times = []\n",
    "    for trial in range(n_eval_trials):\n",
    "        servers = [Server(i, np.random.randint(1,10), np.random.randint(2,20)) \n",
    "                  for i in range(10)]\n",
    "        tasks = generate_tasks(n_tasks)\n",
    "        max_latency = run_simulation(agent, servers, tasks, is_training=False)\n",
    "        eval_times.append(max_latency)\n",
    "        print(f\"Evaluation Trial {trial+1}/{n_eval_trials} | \"\n",
    "              f\"Max Latency: {max_latency:.2f} ms\")\n",
    "\n",
    "    # Final Results\n",
    "    print(\"\\nFinal AIoT-DQN Performance:\")\n",
    "    print(f\"Average Max Latency: {np.mean(eval_times):.2f} ± {np.std(eval_times):.2f} ms\")\n",
    "    print(f\"Best Trial: {np.min(eval_times):.2f} ms\")\n",
    "    print(f\"Worst Trial: {np.max(eval_times):.2f} ms\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
